---
title: "机器学习"
date: 2024-11-12T00:00:00Z
description: "机器学习的基本概念和应用领域。"
math: true
authors:
  - admin
tags:
  - 机器学习
  - 神经网络
  - Markdown
---

# 1. 机器学习总体概念

机器学习是让机器通过数据自动学习规律的技术。  
核心任务是从数据中学习一个函数，用来进行预测或分类。

基本思想：**数据 → 模型 → 学习规则（损失函数 + 优化） → 预测能力**

机器学习主要分为三类：

- **监督学习**：输入 + 标签（分类、回归）
- **无监督学习**：只有输入（聚类、降维、密度估计）
- **强化学习**：智能体通过与环境交互学习策略

---

# 2. 数学基础

机器学习的数学基石包括：

## 2.1 线性代数
- 向量、矩阵
- 矩阵运算（点乘、外积、特征值分解、奇异值分解 SVD）
- 距离与相似度（欧氏距离、余弦相似度）

## 2.2 概率论
- 条件概率
- 贝叶斯公式
- 分布（高斯、多项式）
- 最大似然估计（MLE）

## 2.3 微积分与优化
- 导数与梯度
- 梯度下降（GD）
- 牛顿法、凸优化理论

---

# 3. 监督学习

监督学习以“有标注的数据”为基础，目标是学习输入与输出之间的关系。

## 3.1 分类问题
输入特征 → 输出离散类别。

典型算法：
- 逻辑回归
- 决策树
- 随机森林
- 支持向量机（SVM）
- KNN
- 朴素贝叶斯
- 神经网络（深度学习）

### 逻辑回归（Logistic Regression）
- 用于二分类。
- 本质是线性模型 + Sigmoid 映射成概率。
- 损失函数：交叉熵。

### 决策树
- 通过特征分裂构建树结构。
- 常见分裂指标：信息增益、Gini 指数。

### 随机森林
- 多棵决策树投票产生结果。
- 有效减少过拟合。

### 支持向量机（SVM）
- 寻找最大化间隔的分类超平面。
- 核技巧可处理非线性数据。

---

## 3.2 回归问题
输出连续数值。

典型算法：
- 线性回归
- 岭回归、Lasso
- SVR（支持向量回归）
- GBDT（梯度提升决策树）
- XGBoost / LightGBM

### 线性回归
- 模型：\( y = w^Tx + b \)
- 损失：均方误差（MSE）
- 解法：正规方程或梯度下降。

### Lasso & Ridge
- 通过 L1 或 L2 正则防止过拟合。

---

# 4. 无监督学习

无监督学习处理未标注的数据。

## 4.1 聚类
目标：发现数据内在分组结构。

算法：
- K-Means（基于距离）
- 层次聚类
- DBSCAN（基于密度）

### K-Means 重点
- 随机初始化 K 个中心
- 迭代 “分配 → 更新中心”
- 目标：最小化簇内平方距离

## 4.2 降维
常用于可视化、加速训练、减少噪声。

算法：
- PCA（主成分分析）
- t-SNE（高维可视化）
- Autoencoder（自编码器）

### PCA
通过奇异值分解找到方差最大的方向。  
保留主要成分即可降维。

---

# 5. 模型评估

对训练完的模型需要量化效果。

## 5.1 分类评估指标
- 准确率 Accuracy
- 精确率 Precision
- 召回率 Recall
- F1-score
- ROC 曲线、AUC

## 5.2 回归评估指标
- MSE 均方误差
- MAE 平均绝对误差
- \(R^2\) 决定系数

## 5.3 训练/测试划分
- 训练集（training）
- 验证集（validation）
- 测试集（test）

使用交叉验证（K-fold）提高结果稳定性。

---

# 6. 训练流程（通用机器学习 pipeline）

1. 数据收集
2. 数据清洗（缺失值、异常值）
3. 特征工程
4. 选择模型
5. 训练模型
6. 调参（超参数优化）
7. 模型评估
8. 部署或进一步优化

---

# 7. 特征工程

机器学习的性能很大程度取决于特征。

## 7.1 特征处理方法
- 标准化（Standardization）
- 归一化（Normalization）
- One-hot 编码
- 特征选择（过滤、包裹、嵌入方法）

## 7.2 特征重要性
在树模型、线性模型中尤为重要。

---

# 8. 过拟合与欠拟合

## 8.1 过拟合（记住训练集但泛化差）
解决方法：
- 正则化（L1/L2）
- 数据增强
- 交叉验证
- 降低模型复杂度

## 8.2 欠拟合（模型太简单）
解决方法：
- 使用更复杂模型
- 增加特征
- 减少正则化

---

# 9. 损失函数与优化

损失函数用于度量预测与真实标签的差距。  
优化器用于最小化损失。

## 常见损失
- 回归：MSE、MAE
- 分类：交叉熵

## 常见优化器
- 梯度下降（GD）
- 随机梯度下降（SGD）
- Adam（适应性学习率）

---

# 10. 树模型与 Boosting 系列

树模型在工业界广泛使用。

## 10.1 GBDT（Gradient Boosting Decision Tree）
- 使用残差逐步提升模型表现
- 连续叠加弱学习器（树）

## 10.2 XGBoost
GBDT 的高性能版本：
- 特征并行
- 剪枝算法
- 更快的训练速度

## 10.3 LightGBM
- 基于直方图的高效率算法
- 更快、更省内存
- 适合大数据训练

---

# 11. 强化学习（基础介绍）

强化学习由“智能体”与“环境”组成。

核心概念：
- 状态（State）
- 动作（Action）
- 奖励（Reward）
- 策略（Policy）
- 价值函数（Value Function）

经典方法：
- Q-Learning
- Deep Q Networks (DQN)
- Policy Gradient

---

# 12. 机器学习 vs 深度学习

| 对比项 | 机器学习 | 深度学习 |
|-------|---------|---------|
| 数据需求 | 中等 | 很大 |
| 特征工程 | 需要人工设计 | 自动学习 |
| 训练成本 | 较低 | 高 |
| 模型代表 | SVM、树模型 | CNN、Transformer |
| 适用场景 | 小中型数据、结构化数据 | 图像、文本、大规模数据 |

---