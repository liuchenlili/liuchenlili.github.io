---
title: "深度学习学习笔记"   
date: 2024-11-12T00:00:00Z
description: "探索深度学习中的神经网络及其应用。"
math: true
authors:
  - admin
tags:
  - 深度学习
  - 神经网络
  - Markdown
---

# 深度学习学习笔记

## 1. 深度学习概念概览
- 深度学习属于机器学习的分支，基于多层神经网络。
- 核心目标：通过多层非线性变换自动学习特征表示。
- 深层模型相比手工特征更具表达能力，但训练更依赖数据和计算资源.

---

## 2. 神经网络基础

### 2.1 神经元与计算
- 神经元计算公式：  
  \( y = f(Wx + b) \)  
  其中 \(f\) 为激活函数（ReLU、Sigmoid 等）

### 2.2 激活函数
- **ReLU**：解决梯度消失问题，训练快。
- **Sigmoid / Tanh**：用于概率输出或特定场景。
- **Softmax**：用于多分类。

### 2.3 损失函数
- **分类**：交叉熵（Cross Entropy）
- **回归**：均方误差（MSE）
- 损失函数用于衡量模型输出与真实标签之间的差距。

### 2.4 反向传播与优化器
- 反向传播利用链式法则计算梯度。
- 常见优化器：
    - SGD
    - Adam（自适应学习率）

---

## 3. 常见网络结构

### 3.1 全连接网络（MLP）
- 结构简单，多用于结构化数据或作为其它模型的基础单元。

### 3.2 卷积神经网络（CNN）
- 特点：局部感受野、权重共享。
- 应用：图像分类、检测、分割。
- 经典结构：LeNet、AlexNet、VGG、ResNet。

### 3.3 循环神经网络（RNN / LSTM / GRU）
- 用于处理序列数据（文本、时间序列）。
- LSTM/GRU 解决普通 RNN 的梯度消失问题。

### 3.4 Transformer
- 使用自注意力机制，捕捉任意位置的依赖关系。
- 并行计算效率高，现为 NLP 和 Vision 的主流架构。
- 常见模型：BERT、GPT、ViT。

---

## 4. 深度学习训练流程

1. 准备数据
2. 定义模型结构
3. 定义损失函数
4. 前向传播计算输出
5. 反向传播计算梯度
6. 使用优化器更新参数
7. 在验证集评估效果
8. 调整超参数并迭代训练

---

## 5. 训练常见问题及解决策略

### 5.1 过拟合
- 现象：训练集准确率高，验证集低。
- 解决：
    - 正则化（L2）
    - Dropout
    - 数据增强
    - 使用更小的模型

### 5.2 梯度消失 / 梯度爆炸
- 解决：
    - 使用 ReLU
    - 残差连接（ResNet）
    - 梯度裁剪

### 5.3 学习率问题
- 学习率过大：训练震荡
- 学习率过小：训练很慢且可能陷入局部最优
- 常用策略：学习率衰减、余弦退火、warmup

---

## 6. 参数量、计算量与显存

- 参数量影响模型容量与存储需求。
- FLOPs（浮点运算量）影响计算成本。
- 显存与 batch size 密切相关。
- 混合精度训练（FP16/BF16）可减少显存并加速训练。

---

## 7. 深度学习的主要应用领域

- NLP（文本分类、机器翻译、对话系统）
- CV（图像分类、目标检测、语义分割）
- 多模态（图文、图像生成、视频理解）
- 时间序列预测（金融、天气、传感器）

---

## 8. 学习路径建议

- 数学：线性代数、概率统计、微积分
- 基础编程：Python
- 框架：PyTorch
- 实践：MNIST → CIFAR → Bert → Transformer
- 论文阅读：经典模型结构与优化方法

